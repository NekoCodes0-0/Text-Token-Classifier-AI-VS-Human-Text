{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":52130328,"sourceType":"kernelVersion"},{"sourceId":158617726,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**I'm taking 2 datasets Human text & Ai text and taking some samples and merging them for a small training session.**","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/input\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:19.076762Z","iopub.execute_input":"2025-12-16T06:48:19.077097Z","iopub.status.idle":"2025-12-16T06:48:19.081687Z","shell.execute_reply.started":"2025-12-16T06:48:19.077071Z","shell.execute_reply":"2025-12-16T06:48:19.080855Z"}},"outputs":[{"name":"stdout","text":"['daigt-hc3-dataset', 'starter-wikitext-103-a0194af6-3']\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"from datasets import load_dataset\n\nhuman_ds = load_dataset(\n    \"wikitext\",\n    \"wikitext-103-raw-v1\",\n    split=\"train\"\n)\n\nprint(len(human_ds))\nprint(human_ds[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:19.082921Z","iopub.execute_input":"2025-12-16T06:48:19.083190Z","iopub.status.idle":"2025-12-16T06:48:20.030822Z","shell.execute_reply.started":"2025-12-16T06:48:19.083174Z","shell.execute_reply":"2025-12-16T06:48:20.030240Z"}},"outputs":[{"name":"stdout","text":"1801350\n{'text': ''}\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"text_human = human_ds[10][\"text\"]\n\nprint(text_human[:500])\nprint(\"Word count:\", len(text_human.split()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:20.031632Z","iopub.execute_input":"2025-12-16T06:48:20.031813Z","iopub.status.idle":"2025-12-16T06:48:20.036124Z","shell.execute_reply.started":"2025-12-16T06:48:20.031798Z","shell.execute_reply":"2025-12-16T06:48:20.035428Z"}},"outputs":[{"name":"stdout","text":" The game 's battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limit\nWord count: 296\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"words = text_human.split()\nlabels = [0] * len(words)\n\nprint(\"Words:\", len(words))\nprint(\"Labels:\", len(labels))\nprint(\"First 10 labels:\", labels[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:20.036708Z","iopub.execute_input":"2025-12-16T06:48:20.036873Z","iopub.status.idle":"2025-12-16T06:48:20.052927Z","shell.execute_reply.started":"2025-12-16T06:48:20.036859Z","shell.execute_reply":"2025-12-16T06:48:20.052334Z"}},"outputs":[{"name":"stdout","text":"Words: 296\nLabels: 296\nFirst 10 labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"human_samples = []\n\nfor item in human_ds:\n    text = item[\"text\"]\n    wc = len(text.split())\n\n    if wc < 50:\n        continue\n\n    human_samples.append({\n        \"text\": text,\n        \"labels\": [0] * wc\n    })\n\n    if len(human_samples) == 20:\n        break\n\nprint(\"Human samples created:\", len(human_samples))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:20.054727Z","iopub.execute_input":"2025-12-16T06:48:20.055010Z","iopub.status.idle":"2025-12-16T06:48:20.065288Z","shell.execute_reply.started":"2025-12-16T06:48:20.054988Z","shell.execute_reply":"2025-12-16T06:48:20.064577Z"}},"outputs":[{"name":"stdout","text":"Human samples created: 20\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"import os\nimport json\n\nai_path = \"/kaggle/input/daigt-hc3-dataset\"\n\nprint(os.listdir(ai_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:20.066291Z","iopub.execute_input":"2025-12-16T06:48:20.066549Z","iopub.status.idle":"2025-12-16T06:48:20.077746Z","shell.execute_reply.started":"2025-12-16T06:48:20.066528Z","shell.execute_reply":"2025-12-16T06:48:20.077073Z"}},"outputs":[{"name":"stdout","text":"['__results__.html', '__resultx__.html', '__notebook__.ipynb', '__results___files', '__output__.json', 'train.csv', 'custom.css']\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"import pandas as pd\nimport os\n\nai_path = \"/kaggle/input/daigt-hc3-dataset\"\n\ndf_ai = pd.read_csv(os.path.join(ai_path, \"train.csv\"))\nprint(df_ai.head())\nprint(df_ai.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:20.078454Z","iopub.execute_input":"2025-12-16T06:48:20.078685Z","iopub.status.idle":"2025-12-16T06:48:20.728722Z","shell.execute_reply.started":"2025-12-16T06:48:20.078665Z","shell.execute_reply":"2025-12-16T06:48:20.727939Z"}},"outputs":[{"name":"stdout","text":"                                                text  label\n0  Basically there are many categories of \" Best ...      0\n1  salt is good for not dying in car crashes and ...      0\n2  The way it works is that old TV stations got a...      0\n3  You ca n't just go around assassinating the le...      0\n4  Wanting to kill the shit out of Germans drives...      0\nIndex(['text', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import pandas as pd\nimport os\n\nai_path = \"/kaggle/input/daigt-hc3-dataset\"\n\ndf_ai = pd.read_csv(os.path.join(ai_path, \"train.csv\"))\nprint(df_ai.head())\nprint(df_ai.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:20.729480Z","iopub.execute_input":"2025-12-16T06:48:20.729701Z","iopub.status.idle":"2025-12-16T06:48:21.376008Z","shell.execute_reply.started":"2025-12-16T06:48:20.729684Z","shell.execute_reply":"2025-12-16T06:48:21.375381Z"}},"outputs":[{"name":"stdout","text":"                                                text  label\n0  Basically there are many categories of \" Best ...      0\n1  salt is good for not dying in car crashes and ...      0\n2  The way it works is that old TV stations got a...      0\n3  You ca n't just go around assassinating the le...      0\n4  Wanting to kill the shit out of Germans drives...      0\nIndex(['text', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"print(df_ai[\"text\"].iloc[0][:300])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:21.376708Z","iopub.execute_input":"2025-12-16T06:48:21.377062Z","iopub.status.idle":"2025-12-16T06:48:21.381342Z","shell.execute_reply.started":"2025-12-16T06:48:21.377022Z","shell.execute_reply":"2025-12-16T06:48:21.380690Z"}},"outputs":[{"name":"stdout","text":"Basically there are many categories of \" Best Seller \" . Replace \" Best Seller \" by something like \" Oscars \" and every \" best seller \" book is basically an \" oscar - winning \" book . May not have won the \" Best film \" , but even if you won the best director or best script , you 're still an \" oscar\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"ai_samples = []\n\nfor text in df_ai[\"text\"]:\n    words = text.split()\n    if len(words) < 50:\n        continue\n\n    ai_samples.append({\n        \"text\": text,\n        \"labels\": [1] * len(words)\n    })\n\n    if len(ai_samples) == 20:\n        break\n\nprint(\"AI samples created:\", len(ai_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:21.382079Z","iopub.execute_input":"2025-12-16T06:48:21.382556Z","iopub.status.idle":"2025-12-16T06:48:21.393925Z","shell.execute_reply.started":"2025-12-16T06:48:21.382529Z","shell.execute_reply":"2025-12-16T06:48:21.393313Z"}},"outputs":[{"name":"stdout","text":"AI samples created: 20\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"sample = ai_samples[0]\n\nprint(sample[\"text\"][:300])\nprint(\"Word count:\", len(sample[\"text\"].split()))\nprint(\"First 10 labels:\", sample[\"labels\"][:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:21.394759Z","iopub.execute_input":"2025-12-16T06:48:21.395003Z","iopub.status.idle":"2025-12-16T06:48:21.405497Z","shell.execute_reply.started":"2025-12-16T06:48:21.394977Z","shell.execute_reply":"2025-12-16T06:48:21.404837Z"}},"outputs":[{"name":"stdout","text":"Basically there are many categories of \" Best Seller \" . Replace \" Best Seller \" by something like \" Oscars \" and every \" best seller \" book is basically an \" oscar - winning \" book . May not have won the \" Best film \" , but even if you won the best director or best script , you 're still an \" oscar\nWord count: 267\nFirst 10 labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"**Merging two sample Datasets**","metadata":{}},{"cell_type":"code","source":"import random\n\nall_samples = human_samples + ai_samples\nrandom.shuffle(all_samples)\n\nprint(\"Total samples:\", len(all_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:21.406115Z","iopub.execute_input":"2025-12-16T06:48:21.406306Z","iopub.status.idle":"2025-12-16T06:48:21.417306Z","shell.execute_reply.started":"2025-12-16T06:48:21.406291Z","shell.execute_reply":"2025-12-16T06:48:21.416695Z"}},"outputs":[{"name":"stdout","text":"Total samples: 40\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"Splitting","metadata":{}},{"cell_type":"code","source":"train_samples = all_samples[:32]   # 80%\nval_samples = all_samples[32:]     # 20%\n\nprint(len(train_samples), len(val_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:21.418087Z","iopub.execute_input":"2025-12-16T06:48:21.418691Z","iopub.status.idle":"2025-12-16T06:48:21.429071Z","shell.execute_reply.started":"2025-12-16T06:48:21.418667Z","shell.execute_reply":"2025-12-16T06:48:21.428356Z"}},"outputs":[{"name":"stdout","text":"32 8\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"Loading DE bert A","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:21.432247Z","iopub.execute_input":"2025-12-16T06:48:21.432583Z","iopub.status.idle":"2025-12-16T06:48:22.261170Z","shell.execute_reply.started":"2025-12-16T06:48:21.432568Z","shell.execute_reply":"2025-12-16T06:48:22.260546Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"sample = train_samples[0]\nwords = sample[\"text\"].split()\nword_labels = sample[\"labels\"]\n\nencoding = tokenizer(\n    words,                               # pass list of words\n    is_split_into_words=True,\n    return_offsets_mapping=True,\n    padding=False,\n    truncation=True,\n    max_length=512                        # DeBERTa limit\n)\n\ntokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\nword_ids = encoding.word_ids()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.262231Z","iopub.execute_input":"2025-12-16T06:48:22.262418Z","iopub.status.idle":"2025-12-16T06:48:22.267491Z","shell.execute_reply.started":"2025-12-16T06:48:22.262404Z","shell.execute_reply":"2025-12-16T06:48:22.266938Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"print(\"WORDS:\", words[:10])\nprint(\"TOKENS:\", tokens[:20])\nprint(\"WORD IDs:\", word_ids[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.268161Z","iopub.execute_input":"2025-12-16T06:48:22.268434Z","iopub.status.idle":"2025-12-16T06:48:22.281466Z","shell.execute_reply.started":"2025-12-16T06:48:22.268416Z","shell.execute_reply":"2025-12-16T06:48:22.280910Z"}},"outputs":[{"name":"stdout","text":"WORDS: ['On', 'its', 'day', 'of', 'release', 'in', 'Japan', ',', 'Valkyria', 'Chronicles']\nTOKENS: ['[CLS]', 'â–On', 'â–its', 'â–day', 'â–of', 'â–release', 'â–in', 'â–Japan', 'â–,', 'â–Valk', 'yria', 'â–Chronicles', 'â–III', 'â–topped', 'â–both', 'â–platform', 'â–@', '-', '@', 'â–exclusive']\nWORD IDs: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 14, 14, 15]\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"def align_labels_with_tokens(word_labels, word_ids):\n    token_labels = []\n    previous_word = None\n    \n    for word_id in word_ids:\n        if word_id is None:  \n            token_labels.append(-100)  # ignore special tokens\n        elif word_id != previous_word:\n            token_labels.append(word_labels[word_id])  # first token of a word\n            previous_word = word_id\n        else:\n            token_labels.append(-100)  # ignore subsequent subword tokens\n\n    return token_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.282171Z","iopub.execute_input":"2025-12-16T06:48:22.282394Z","iopub.status.idle":"2025-12-16T06:48:22.294952Z","shell.execute_reply.started":"2025-12-16T06:48:22.282374Z","shell.execute_reply":"2025-12-16T06:48:22.294372Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"token_labels = align_labels_with_tokens(word_labels, word_ids)\n\nprint(\"Token labels:\", token_labels[:20])\nprint(\"Tokens:\", tokens[:20])\nprint(\"Word IDs:\", word_ids[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.295651Z","iopub.execute_input":"2025-12-16T06:48:22.295951Z","iopub.status.idle":"2025-12-16T06:48:22.308453Z","shell.execute_reply.started":"2025-12-16T06:48:22.295935Z","shell.execute_reply":"2025-12-16T06:48:22.307819Z"}},"outputs":[{"name":"stdout","text":"Token labels: [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, -100, 0]\nTokens: ['[CLS]', 'â–On', 'â–its', 'â–day', 'â–of', 'â–release', 'â–in', 'â–Japan', 'â–,', 'â–Valk', 'yria', 'â–Chronicles', 'â–III', 'â–topped', 'â–both', 'â–platform', 'â–@', '-', '@', 'â–exclusive']\nWord IDs: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 14, 14, 15]\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"class DebertaTokenDataset:\n    def __init__(self, samples, tokenizer, max_length=512):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        text = self.samples[idx][\"text\"]\n        word_labels = self.samples[idx][\"labels\"]\n        words = text.split()\n        \n        encoding = self.tokenizer(\n            words,\n            is_split_into_words=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length\n        )\n        \n        word_ids = encoding.word_ids()\n        token_labels = align_labels_with_tokens(word_labels, word_ids)\n        \n        # convert to tensors\n        item = {key: torch.tensor(val) for key, val in encoding.items() if key != \"offset_mapping\"}\n        item[\"labels\"] = torch.tensor(token_labels)\n        \n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.309156Z","iopub.execute_input":"2025-12-16T06:48:22.309332Z","iopub.status.idle":"2025-12-16T06:48:22.322191Z","shell.execute_reply.started":"2025-12-16T06:48:22.309319Z","shell.execute_reply":"2025-12-16T06:48:22.321434Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"train_dataset = DebertaTokenDataset(train_samples, tokenizer)\nval_dataset = DebertaTokenDataset(val_samples, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.322965Z","iopub.execute_input":"2025-12-16T06:48:22.323318Z","iopub.status.idle":"2025-12-16T06:48:22.413129Z","shell.execute_reply.started":"2025-12-16T06:48:22.323296Z","shell.execute_reply":"2025-12-16T06:48:22.412406Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.413929Z","iopub.execute_input":"2025-12-16T06:48:22.414221Z","iopub.status.idle":"2025-12-16T06:48:22.424365Z","shell.execute_reply.started":"2025-12-16T06:48:22.414199Z","shell.execute_reply":"2025-12-16T06:48:22.423761Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"item = train_dataset[0]\n\nprint(item.keys())\nprint(\"input_ids shape:\", item[\"input_ids\"].shape)\nprint(\"labels shape:\", item[\"labels\"].shape)\nprint(item[\"labels\"][:40])   # show first 40 token labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.425116Z","iopub.execute_input":"2025-12-16T06:48:22.425308Z","iopub.status.idle":"2025-12-16T06:48:22.437635Z","shell.execute_reply.started":"2025-12-16T06:48:22.425294Z","shell.execute_reply":"2025-12-16T06:48:22.437076Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\ninput_ids shape: torch.Size([512])\nlabels shape: torch.Size([512])\ntensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,    0,\n           0,    0,    0,    0,    0, -100, -100,    0,    0,    0,    0, -100,\n        -100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0, -100, -100])\n","output_type":"stream"}],"execution_count":89},{"cell_type":"markdown","source":"# **All the steps above was to prepare a small dataset to test and mimic a small portion of real work! This is not the real output but a demonstration of how your output will looklike according to the document you provided me**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.438289Z","iopub.execute_input":"2025-12-16T06:48:22.438482Z","iopub.status.idle":"2025-12-16T06:48:22.446226Z","shell.execute_reply.started":"2025-12-16T06:48:22.438458Z","shell.execute_reply":"2025-12-16T06:48:22.445574Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.446830Z","iopub.execute_input":"2025-12-16T06:48:22.447059Z","iopub.status.idle":"2025-12-16T06:48:22.485141Z","shell.execute_reply.started":"2025-12-16T06:48:22.447018Z","shell.execute_reply":"2025-12-16T06:48:22.484572Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    \"microsoft/deberta-v3-large\",\n    num_labels=2,\n    ignore_mismatched_sizes=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:22.485816Z","iopub.execute_input":"2025-12-16T06:48:22.486069Z","iopub.status.idle":"2025-12-16T06:48:24.132213Z","shell.execute_reply.started":"2025-12-16T06:48:22.486047Z","shell.execute_reply":"2025-12-16T06:48:24.131575Z"}},"outputs":[{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:24.132853Z","iopub.execute_input":"2025-12-16T06:48:24.133054Z","iopub.status.idle":"2025-12-16T06:48:24.139860Z","shell.execute_reply.started":"2025-12-16T06:48:24.133017Z","shell.execute_reply":"2025-12-16T06:48:24.139159Z"}},"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"DebertaV2ForTokenClassification(\n  (deberta): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":93},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./deberta_detector\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    fp16=True,\n    logging_steps=10,\n    weight_decay=0.01,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:24.140702Z","iopub.execute_input":"2025-12-16T06:48:24.141123Z","iopub.status.idle":"2025-12-16T06:48:24.180714Z","shell.execute_reply.started":"2025-12-16T06:48:24.141099Z","shell.execute_reply":"2025-12-16T06:48:24.180155Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta_detector\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    fp16=True,\n    logging_steps=10,\n    weight_decay=0.01,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:24.181518Z","iopub.execute_input":"2025-12-16T06:48:24.181747Z","iopub.status.idle":"2025-12-16T06:48:24.215276Z","shell.execute_reply.started":"2025-12-16T06:48:24.181731Z","shell.execute_reply":"2025-12-16T06:48:24.214761Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta_detector\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    fp16=True,\n    logging_steps=10,\n    weight_decay=0.01,\n    report_to=\"none\"   # ðŸ”´ THIS IS IMPORTANT\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:24.215888Z","iopub.execute_input":"2025-12-16T06:48:24.216085Z","iopub.status.idle":"2025-12-16T06:48:24.246546Z","shell.execute_reply.started":"2025-12-16T06:48:24.216069Z","shell.execute_reply":"2025-12-16T06:48:24.246059Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:24.247142Z","iopub.execute_input":"2025-12-16T06:48:24.247366Z","iopub.status.idle":"2025-12-16T06:48:24.783214Z","shell.execute_reply.started":"2025-12-16T06:48:24.247342Z","shell.execute_reply":"2025-12-16T06:48:24.782390Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/1652196004.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:48:24.784254Z","iopub.execute_input":"2025-12-16T06:48:24.784533Z","iopub.status.idle":"2025-12-16T06:49:01.059844Z","shell.execute_reply.started":"2025-12-16T06:48:24.784509Z","shell.execute_reply":"2025-12-16T06:49:01.059087Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4, training_loss=0.42701753973960876, metrics={'train_runtime': 35.6595, 'train_samples_per_second': 1.795, 'train_steps_per_second': 0.112, 'total_flos': 59437650149376.0, 'train_loss': 0.42701753973960876, 'epoch': 2.0})"},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:01.060716Z","iopub.execute_input":"2025-12-16T06:49:01.061023Z","iopub.status.idle":"2025-12-16T06:49:02.563730Z","shell.execute_reply.started":"2025-12-16T06:49:01.060994Z","shell.execute_reply":"2025-12-16T06:49:02.562959Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:01]\n    </div>\n    "},"metadata":{}},{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.14662860333919525,\n 'eval_runtime': 1.4943,\n 'eval_samples_per_second': 5.354,\n 'eval_steps_per_second': 2.677,\n 'epoch': 2.0}"},"metadata":{}}],"execution_count":99},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef predict_batch(texts, model, tokenizer, device=\"cuda\"):\n    model.eval()\n    model.to(device)\n\n    all_outputs = []\n\n    with torch.no_grad():\n        for text in texts:\n            words = text.split()\n\n            encoding = tokenizer(\n                words,\n                is_split_into_words=True,\n                return_offsets_mapping=True,\n                truncation=True,\n                max_length=512,\n                return_tensors=\"pt\"\n            )\n\n            word_ids = encoding.word_ids()\n            encoding = {k: v.to(device) for k, v in encoding.items() if k != \"offset_mapping\"}\n\n            outputs = model(**encoding)\n            logits = outputs.logits.squeeze(0)      # [seq_len, 2]\n            probs = torch.softmax(logits, dim=-1)[:, 1]  # AI probability\n\n            # Aggregate token probs â†’ word probs\n            word_probs = {}\n            for token_idx, word_id in enumerate(word_ids):\n                if word_id is None:\n                    continue\n                word_probs.setdefault(word_id, []).append(probs[token_idx].item())\n\n            # Use MAX over subwords (recommended)\n            final_probs = [max(word_probs[i]) for i in range(len(words))]\n            all_outputs.append(final_probs)\n\n    return all_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:02.564541Z","iopub.execute_input":"2025-12-16T06:49:02.564797Z","iopub.status.idle":"2025-12-16T06:49:07.259857Z","shell.execute_reply.started":"2025-12-16T06:49:02.564779Z","shell.execute_reply":"2025-12-16T06:49:07.259083Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"# Example texts\ntexts = [\n    \"This is a simple human written sentence.\",\n    \"This text was generated by an artificial intelligence model.\"\n]\n\noutputs = predict_batch(texts, model, tokenizer)\n\nprint(outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:07.260712Z","iopub.execute_input":"2025-12-16T06:49:07.260994Z","iopub.status.idle":"2025-12-16T06:49:09.070635Z","shell.execute_reply.started":"2025-12-16T06:49:07.260971Z","shell.execute_reply":"2025-12-16T06:49:09.069977Z"}},"outputs":[{"name":"stdout","text":"[[0.3232060670852661, 0.31152811646461487, 0.23936733603477478, 0.5322992205619812, 0.4280407726764679, 0.4280068278312683, 0.37719348073005676], [0.1329631358385086, 0.22803784906864166, 0.06984741985797882, 0.07881484180688858, 0.08180958032608032, 0.06731610000133514, 0.09023510664701462, 0.13098254799842834, 0.1381264179944992]]\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"words = texts[0].split()\nprint(words)\nprint(outputs[0])\nprint(len(words), len(outputs[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:09.071403Z","iopub.execute_input":"2025-12-16T06:49:09.071737Z","iopub.status.idle":"2025-12-16T06:49:10.581542Z","shell.execute_reply.started":"2025-12-16T06:49:09.071718Z","shell.execute_reply":"2025-12-16T06:49:10.580676Z"}},"outputs":[{"name":"stdout","text":"['This', 'is', 'a', 'simple', 'human', 'written', 'sentence.']\n[0.3232060670852661, 0.31152811646461487, 0.23936733603477478, 0.5322992205619812, 0.4280407726764679, 0.4280068278312683, 0.37719348073005676]\n7 7\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"MAX_HUMAN = 10_000\nhuman_samples = []\n\nfor item in human_ds:\n    text = item[\"text\"]\n    words = text.split()\n\n    if len(words) < 80 or len(words) > 400:\n        continue\n\n    human_samples.append({\n        \"text\": text,\n        \"labels\": [0] * len(words)\n    })\n\n    if len(human_samples) >= MAX_HUMAN:\n        break\n\nprint(\"Human samples:\", len(human_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:10.582371Z","iopub.execute_input":"2025-12-16T06:49:10.582552Z","iopub.status.idle":"2025-12-16T06:49:12.461671Z","shell.execute_reply.started":"2025-12-16T06:49:10.582537Z","shell.execute_reply":"2025-12-16T06:49:12.461052Z"}},"outputs":[{"name":"stdout","text":"Human samples: 10000\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"MAX_AI = 10_000\nai_samples = []\n\nfor text in df_ai[\"text\"]:   # or \"answer\"\n    words = text.split()\n\n    if len(words) < 80 or len(words) > 400:\n        continue\n\n    ai_samples.append({\n        \"text\": text,\n        \"labels\": [1] * len(words)\n    })\n\n    if len(ai_samples) >= MAX_AI:\n        break\n\nprint(\"AI samples:\", len(ai_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:12.462294Z","iopub.execute_input":"2025-12-16T06:49:12.462477Z","iopub.status.idle":"2025-12-16T06:49:12.965379Z","shell.execute_reply.started":"2025-12-16T06:49:12.462463Z","shell.execute_reply":"2025-12-16T06:49:12.964569Z"}},"outputs":[{"name":"stdout","text":"AI samples: 10000\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef predict_batch(texts, model, tokenizer, device=\"cuda\"):\n    model.eval()\n    model.to(device)\n\n    all_outputs = []\n\n    with torch.no_grad():\n        for text in texts:\n            words = text.split()\n\n            encoding = tokenizer(\n                words,\n                is_split_into_words=True,\n                return_offsets_mapping=True,\n                truncation=True,\n                max_length=512,\n                return_tensors=\"pt\"\n            )\n\n            word_ids = encoding.word_ids()\n            encoding = {k: v.to(device) for k, v in encoding.items() if k != \"offset_mapping\"}\n\n            outputs = model(**encoding)\n            logits = outputs.logits.squeeze(0)      # [seq_len, 2]\n            probs = torch.softmax(logits, dim=-1)[:, 1]  # AI probability\n\n            # Aggregate token probs â†’ word probs\n            word_probs = {}\n            for token_idx, word_id in enumerate(word_ids):\n                if word_id is None:\n                    continue\n                word_probs.setdefault(word_id, []).append(probs[token_idx].item())\n\n            # Use MAX over subwords (recommended)\n            final_probs = [max(word_probs[i]) for i in range(len(words))]\n            all_outputs.append(final_probs)\n\n    return all_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:12.966200Z","iopub.execute_input":"2025-12-16T06:49:12.966403Z","iopub.status.idle":"2025-12-16T06:49:14.183958Z","shell.execute_reply.started":"2025-12-16T06:49:12.966386Z","shell.execute_reply":"2025-12-16T06:49:14.183097Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"# Example texts\ntexts = [\n    \"This is a simple human written sentence.\",\n    \"This text was generated by an artificial intelligence model.\"\n]\n\noutputs = predict_batch(texts, model, tokenizer)\n\nprint(outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:14.184803Z","iopub.execute_input":"2025-12-16T06:49:14.185272Z","iopub.status.idle":"2025-12-16T06:49:19.237696Z","shell.execute_reply.started":"2025-12-16T06:49:14.185253Z","shell.execute_reply":"2025-12-16T06:49:19.237070Z"}},"outputs":[{"name":"stdout","text":"[[0.3232060670852661, 0.31152811646461487, 0.23936733603477478, 0.5322992205619812, 0.4280407726764679, 0.4280068278312683, 0.37719348073005676], [0.1329631358385086, 0.22803784906864166, 0.06984741985797882, 0.07881484180688858, 0.08180958032608032, 0.06731610000133514, 0.09023510664701462, 0.13098254799842834, 0.1381264179944992]]\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"import torch, gc\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"Memory cleared\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:19.238349Z","iopub.execute_input":"2025-12-16T06:49:19.238535Z","iopub.status.idle":"2025-12-16T06:49:19.984186Z","shell.execute_reply.started":"2025-12-16T06:49:19.238512Z","shell.execute_reply":"2025-12-16T06:49:19.983416Z"}},"outputs":[{"name":"stdout","text":"Memory cleared\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"import random\n\nmixed_human_ai = []\n\nfor _ in range(800):\n    h = random.choice(human_samples)\n    a = random.choice(ai_samples)\n\n    h_words = h[\"text\"].split()\n    a_words = a[\"text\"].split()\n\n    cut = int(0.6 * len(h_words))  # human first\n\n    words = h_words[:cut] + a_words[:len(h_words) - cut]\n    labels = [0]*cut + [1]*(len(words) - cut)\n\n    mixed_human_ai.append({\n        \"text\": \" \".join(words),\n        \"labels\": labels\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:49:19.987903Z","iopub.execute_input":"2025-12-16T06:49:19.988142Z","iopub.status.idle":"2025-12-16T06:49:20.019559Z","shell.execute_reply.started":"2025-12-16T06:49:19.988124Z","shell.execute_reply":"2025-12-16T06:49:20.018867Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"val_texts = [s[\"text\"] for s in val_samples]\n\n# sentence label: 1 if any AI word exists, else 0\nval_labels = [1 if any(l == 1 for l in s[\"labels\"]) else 0 for s in val_samples]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:53:34.994193Z","iopub.execute_input":"2025-12-16T06:53:34.994749Z","iopub.status.idle":"2025-12-16T06:53:34.999072Z","shell.execute_reply.started":"2025-12-16T06:53:34.994715Z","shell.execute_reply":"2025-12-16T06:53:34.998382Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"def sentence_scores(texts, model, tokenizer):\n    outputs = predict_batch(texts, model, tokenizer)\n    return [sum(p) / len(p) for p in outputs]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:54:09.962490Z","iopub.execute_input":"2025-12-16T06:54:09.963278Z","iopub.status.idle":"2025-12-16T06:54:09.967106Z","shell.execute_reply.started":"2025-12-16T06:54:09.963251Z","shell.execute_reply":"2025-12-16T06:54:09.966397Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"scores = sentence_scores(val_texts, model, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:54:19.863544Z","iopub.execute_input":"2025-12-16T06:54:19.863826Z","iopub.status.idle":"2025-12-16T06:54:20.824916Z","shell.execute_reply.started":"2025-12-16T06:54:19.863803Z","shell.execute_reply":"2025-12-16T06:54:20.824330Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"import numpy as np\n\nbest_acc = 0.0\nbest_threshold = 0.5\n\nfor t in np.arange(0.1, 0.9, 0.01):\n    preds = [1 if s >= t else 0 for s in scores]\n    acc = sum(p == y for p, y in zip(preds, val_labels)) / len(val_labels)\n\n    if acc > best_acc:\n        best_acc = acc\n        best_threshold = t\n\nprint(f\"Best Threshold: {best_threshold:.2f}\")\nprint(f\"Validation Accuracy: {best_acc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:54:45.190850Z","iopub.execute_input":"2025-12-16T06:54:45.191174Z","iopub.status.idle":"2025-12-16T06:54:45.197071Z","shell.execute_reply.started":"2025-12-16T06:54:45.191148Z","shell.execute_reply":"2025-12-16T06:54:45.196358Z"}},"outputs":[{"name":"stdout","text":"Best Threshold: 0.15\nValidation Accuracy: 1.000\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"import random\n\nmixed_human_ai = []\n\nfor _ in range(800):\n    h = random.choice(human_samples)\n    a = random.choice(ai_samples)\n\n    h_words = h[\"text\"].split()\n    a_words = a[\"text\"].split()\n\n    cut = int(0.6 * len(h_words))  # human first\n\n    words = h_words[:cut] + a_words[:len(h_words) - cut]\n    labels = [0]*cut + [1]*(len(words) - cut)\n\n    mixed_human_ai.append({\n        \"text\": \" \".join(words),\n        \"labels\": labels\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:55:53.256919Z","iopub.execute_input":"2025-12-16T06:55:53.257621Z","iopub.status.idle":"2025-12-16T06:55:53.292525Z","shell.execute_reply.started":"2025-12-16T06:55:53.257594Z","shell.execute_reply":"2025-12-16T06:55:53.291799Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"mixed_ai_middle = []\n\nfor _ in range(200):\n    h = random.choice(human_samples)\n    a = random.choice(ai_samples)\n\n    h_words = h[\"text\"].split()\n    a_words = a[\"text\"].split()\n\n    start = len(h_words) // 3\n    end = start + len(h_words) // 3\n\n    words = (\n        h_words[:start]\n        + a_words[: end - start]\n        + h_words[end:]\n    )\n\n    labels = (\n        [0]*start\n        + [1]*(end - start)\n        + [0]*(len(words) - end)\n    )\n\n    mixed_ai_middle.append({\n        \"text\": \" \".join(words),\n        \"labels\": labels\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:56:03.346687Z","iopub.execute_input":"2025-12-16T06:56:03.346950Z","iopub.status.idle":"2025-12-16T06:56:03.359140Z","shell.execute_reply.started":"2025-12-16T06:56:03.346932Z","shell.execute_reply":"2025-12-16T06:56:03.358402Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"human_small = human_samples[:500]\nai_small = ai_samples[:500]\n\nall_samples_small = (\n    human_small\n    + ai_small\n    + mixed_human_ai\n    + mixed_ai_middle\n)\n\nrandom.shuffle(all_samples_small)\n\nsplit = int(0.8 * len(all_samples_small))\ntrain_samples = all_samples_small[:split]\nval_samples = all_samples_small[split:]\n\nprint(len(train_samples), len(val_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:56:50.845188Z","iopub.execute_input":"2025-12-16T06:56:50.845950Z","iopub.status.idle":"2025-12-16T06:56:50.854424Z","shell.execute_reply.started":"2025-12-16T06:56:50.845912Z","shell.execute_reply":"2025-12-16T06:56:50.853734Z"}},"outputs":[{"name":"stdout","text":"1600 400\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"import random\n\n# sizes for SMALL-SCALE tuning\nN_HUMAN = 500\nN_AI = 500\nN_MIX_HAI = 800     # Human â†’ AI\nN_MIX_MID = 200     # AI in middle\n\n# 1) pure samples\nhuman_small = human_samples[:N_HUMAN]\nai_small = ai_samples[:N_AI]\n\n# 2) mixed: Human â†’ AI (40%)\nmixed_human_ai = []\nfor _ in range(N_MIX_HAI):\n    h = random.choice(human_samples)\n    a = random.choice(ai_samples)\n\n    h_words = h[\"text\"].split()\n    a_words = a[\"text\"].split()\n\n    cut = int(0.6 * len(h_words))\n    words = h_words[:cut] + a_words[:len(h_words)-cut]\n    labels = [0]*cut + [1]*(len(words)-cut)\n\n    mixed_human_ai.append({\"text\": \" \".join(words), \"labels\": labels})\n\n# 3) mixed: AI in the middle (10%)\nmixed_ai_middle = []\nfor _ in range(N_MIX_MID):\n    h = random.choice(human_samples)\n    a = random.choice(ai_samples)\n\n    h_words = h[\"text\"].split()\n    a_words = a[\"text\"].split()\n\n    start = len(h_words)//3\n    end = start + len(h_words)//3\n\n    words = h_words[:start] + a_words[:end-start] + h_words[end:]\n    labels = [0]*start + [1]*(end-start) + [0]*(len(words)-end)\n\n    mixed_ai_middle.append({\"text\": \" \".join(words), \"labels\": labels})\n\n# combine\nall_samples_small = (\n    human_small + ai_small + mixed_human_ai + mixed_ai_middle\n)\n\nrandom.shuffle(all_samples_small)\n\nprint(\"Total small samples:\", len(all_samples_small))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:57:21.334319Z","iopub.execute_input":"2025-12-16T06:57:21.335008Z","iopub.status.idle":"2025-12-16T06:57:21.378380Z","shell.execute_reply.started":"2025-12-16T06:57:21.334980Z","shell.execute_reply":"2025-12-16T06:57:21.377632Z"}},"outputs":[{"name":"stdout","text":"Total small samples: 2000\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"# 80 / 20 split\nsplit = int(0.8 * len(all_samples_small))\n\ntrain_samples = all_samples_small[:split]\nval_samples = all_samples_small[split:]\n\nprint(\"Train samples:\", len(train_samples))\nprint(\"Val samples:\", len(val_samples))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:57:34.044818Z","iopub.execute_input":"2025-12-16T06:57:34.045124Z","iopub.status.idle":"2025-12-16T06:57:34.051168Z","shell.execute_reply.started":"2025-12-16T06:57:34.045101Z","shell.execute_reply":"2025-12-16T06:57:34.050505Z"}},"outputs":[{"name":"stdout","text":"Train samples: 1600\nVal samples: 400\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"train_dataset = DebertaTokenDataset(train_samples, tokenizer)\nval_dataset = DebertaTokenDataset(val_samples, tokenizer)\n\nprint(\"Datasets created\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:57:44.497854Z","iopub.execute_input":"2025-12-16T06:57:44.498147Z","iopub.status.idle":"2025-12-16T06:57:44.502514Z","shell.execute_reply.started":"2025-12-16T06:57:44.498124Z","shell.execute_reply":"2025-12-16T06:57:44.501840Z"}},"outputs":[{"name":"stdout","text":"Datasets created\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta_small_tune\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,   # effective batch = 8\n    num_train_epochs=5,              # more epochs for small data\n    learning_rate=1e-5,              # stable learning\n    fp16=True,\n    logging_steps=20,\n    save_total_limit=1,\n    report_to=\"none\"\n)\n\nprint(\"TrainingArguments ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:57:53.616233Z","iopub.execute_input":"2025-12-16T06:57:53.616475Z","iopub.status.idle":"2025-12-16T06:57:53.650146Z","shell.execute_reply.started":"2025-12-16T06:57:53.616459Z","shell.execute_reply":"2025-12-16T06:57:53.649251Z"}},"outputs":[{"name":"stdout","text":"TrainingArguments ready\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\nprint(\"Trainer ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:58:15.059957Z","iopub.execute_input":"2025-12-16T06:58:15.060624Z","iopub.status.idle":"2025-12-16T06:58:15.085290Z","shell.execute_reply.started":"2025-12-16T06:58:15.060601Z","shell.execute_reply":"2025-12-16T06:58:15.084709Z"}},"outputs":[{"name":"stdout","text":"Trainer ready\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2925376803.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":123},{"cell_type":"code","source":"import torch, gc\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"Memory cleared\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:06:13.838276Z","iopub.execute_input":"2025-12-16T07:06:13.838566Z","iopub.status.idle":"2025-12-16T07:06:14.376131Z","shell.execute_reply.started":"2025-12-16T07:06:13.838544Z","shell.execute_reply":"2025-12-16T07:06:14.375483Z"}},"outputs":[{"name":"stdout","text":"Memory cleared\n","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"microsoft/deberta-v3-base\",\n    num_labels=2,\n    ignore_mismatched_sizes=True\n)\n\nprint(\"DeBERTa-base loaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:06:26.074422Z","iopub.execute_input":"2025-12-16T07:06:26.074685Z","iopub.status.idle":"2025-12-16T07:06:28.964165Z","shell.execute_reply.started":"2025-12-16T07:06:26.074665Z","shell.execute_reply":"2025-12-16T07:06:28.963562Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"989113c1b8b345bdbe3176701aec01e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40127a96a4d4fa1a5fd56e8f3f3c104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6bbd83558747fd865c24dcf793cfc6"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"DeBERTa-base loaded\n","output_type":"stream"}],"execution_count":142},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta_small_tune\",\n    per_device_train_batch_size=2,   # base allows this\n    num_train_epochs=5,\n    learning_rate=2e-5,\n    fp16=True,\n    logging_steps=20,\n    save_total_limit=1,\n    report_to=\"none\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:07:00.337606Z","iopub.execute_input":"2025-12-16T07:07:00.338258Z","iopub.status.idle":"2025-12-16T07:07:00.369883Z","shell.execute_reply.started":"2025-12-16T07:07:00.338231Z","shell.execute_reply":"2025-12-16T07:07:00.369371Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\nprint(\"Trainer ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:07:14.040457Z","iopub.execute_input":"2025-12-16T07:07:14.040716Z","iopub.status.idle":"2025-12-16T07:07:14.254918Z","shell.execute_reply.started":"2025-12-16T07:07:14.040698Z","shell.execute_reply":"2025-12-16T07:07:14.254344Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2925376803.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Trainer ready\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:07:25.499426Z","iopub.execute_input":"2025-12-16T07:07:25.500222Z","iopub.status.idle":"2025-12-16T07:24:32.545761Z","shell.execute_reply.started":"2025-12-16T07:07:25.500193Z","shell.execute_reply":"2025-12-16T07:24:32.545194Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 17:06, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.683800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.228500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.169500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.110900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.316200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.195300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.197700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.111700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.120700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.203800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.335500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.149600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.209700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.070300</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.064400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.151500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.039700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.139900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.010600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.183700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.089900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.054300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.234100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.045300</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.091800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.002400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.053800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.095500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.099600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.065000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.067700</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.028100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.078900</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.153300</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.027100</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.037600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.009000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.031000</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.051396172103665864, metrics={'train_runtime': 1026.5428, 'train_samples_per_second': 7.793, 'train_steps_per_second': 1.948, 'total_flos': 2090411802624000.0, 'train_loss': 0.051396172103665864, 'epoch': 5.0})"},"metadata":{}}],"execution_count":146},{"cell_type":"code","source":"val_texts = [s[\"text\"] for s in val_samples]\n\n# Sentence label: AI if any AI word exists\nval_labels = [\n    1 if any(l == 1 for l in s[\"labels\"]) else 0\n    for s in val_samples\n]\n\nprint(\"Validation samples:\", len(val_texts))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:27:02.938529Z","iopub.execute_input":"2025-12-16T07:27:02.938803Z","iopub.status.idle":"2025-12-16T07:27:02.945516Z","shell.execute_reply.started":"2025-12-16T07:27:02.938780Z","shell.execute_reply":"2025-12-16T07:27:02.944885Z"}},"outputs":[{"name":"stdout","text":"Validation samples: 400\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"def sentence_scores(texts, model, tokenizer):\n    outputs = predict_batch(texts, model, tokenizer)\n    return [sum(p) / len(p) for p in outputs]\n\nscores = sentence_scores(val_texts, model, tokenizer)\nprint(\"Scores computed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:27:16.079480Z","iopub.execute_input":"2025-12-16T07:27:16.080222Z","iopub.status.idle":"2025-12-16T07:27:27.596091Z","shell.execute_reply.started":"2025-12-16T07:27:16.080192Z","shell.execute_reply":"2025-12-16T07:27:27.595262Z"}},"outputs":[{"name":"stdout","text":"Scores computed\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"import numpy as np\n\nbest_acc = 0.0\nbest_t = 0.5\n\nfor t in np.arange(0.1, 0.9, 0.01):\n    preds = [1 if s >= t else 0 for s in scores]\n    acc = sum(p == y for p, y in zip(preds, val_labels)) / len(val_labels)\n\n    if acc > best_acc:\n        best_acc = acc\n        best_t = t\n\nprint(f\"Best Threshold: {best_t:.2f}\")\nprint(f\"Validation Accuracy: {best_acc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:27:31.033976Z","iopub.execute_input":"2025-12-16T07:27:31.034698Z","iopub.status.idle":"2025-12-16T07:27:31.043938Z","shell.execute_reply.started":"2025-12-16T07:27:31.034673Z","shell.execute_reply":"2025-12-16T07:27:31.043321Z"}},"outputs":[{"name":"stdout","text":"Best Threshold: 0.10\nValidation Accuracy: 1.000\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}